---
layout: post
title:  "一致性Hash问题及解决方案、分布式集群时钟同步问题及解决方案、分布式ID问题及解决方案"
date:   2021-06-10
categories: lagou
tags: lagou
---

* content
{:toc}


1. 第二阶段、Web服务器深度应用及调优
2. 第二模块、Cluster模式潜在问题及解决方案、Web服务综合解决方案
3. 任务1、一致性Hash问题及解决方案
4. 任务2、分布式集群时钟同步问题及解决方案
5. 任务3、分布式ID问题及解决方案
  





 
 
# 第⼀部分 ⼀致性Hash算法
## 1.1 Hash算法应⽤场景
Hash算法在分布式集群架构中的应⽤场景

Hash算法在很多分布式集群产品中都有应⽤，⽐如分布式集群架构Redis、Hadoop、ElasticSearch、Mysql分库分表、Nginx负载均衡等

主要的应⽤场景归纳起来两个

**1.请求的负载均衡（⽐如nginx的ip_hash策略）**
>Nginx的IP_hash策略可以在客户端ip不变的情况下，将其发出的请求始终路由到同⼀个⽬标服务器上，实现会话粘滞，避免处理session共享问题  
>如果没有IP_hash策略，那么如何实现会话粘滞？  
>可以维护⼀张映射表，存储客户端IP或者sessionid与具体⽬标服务器的映射关系  
>
>缺点  
>那么，在客户端很多的情况下，映射表⾮常⼤，浪费内存空间  
>客户端上下线，⽬标服务器上下线，都会导致重新维护映射表，映射表维护成本很⼤  

如果使⽤哈希算法，事情就简单很多，我们可以对ip地址或者sessionid进⾏计算哈希值，哈希值与服务器数量进⾏取模运算，得到的值就是当前请求应该被路由到的服务器编号，如此，同⼀个客户端ip发送过来的请求就可以路由到同⼀个⽬标服务器，实现会话粘滞

**2.分布式存储**  
>以分布式内存数据库Redis为例,集群中有redis1， redis2， redis3 三台Redis服务器  
>那么,在进⾏数据存储时,<key1,value1>数据存储到哪个服务器当中呢？针对key进⾏hash处理hash(key1)%3=index, 使⽤余数index锁定存储的具体服务器节点

## 1.2 普通Hash算法存在的问题
普通Hash算法存在⼀个问题，以ip_hash为例，假定下载⽤户ip固定没有发⽣改变，现在tomcat3出现了问题， down机了，服务器数量由3个变为了2个，之前所有的求模都需要重新计算。  

![普通Hash](/assets/lagou/第二阶段/02.第二模块/普通Hash.jpg)

如果在真实⽣产情况下，后台服务器很多台，客户端也有很多，那么影响是很⼤的，缩容和扩容都会存在这样的问题，⼤量⽤户的请求会被路由到其他的⽬标服务器处理，⽤户在原来服务器中的会话都会丢失。

## 1.3 ⼀致性Hash算法
⼀致性哈希算法思路如下：  
![⼀致性哈希算法](/assets/lagou/第二阶段/02.第二模块/⼀致性哈希算法.jpg)

⾸先有⼀条直线，直线开头和结尾分别定为为1和2的32次⽅减1，这相当于⼀个地址，对于这样⼀条线，弯过来构成⼀个圆环形成闭环，这样的⼀个圆环称为hash环。我们把服务器的ip或者主机名求hash值然后对应到hash环上，那么针对客户端⽤户，也根据它的ip进⾏hash求值，对应到环上某个位置，然后如何确定⼀个客户端路由到哪个服务器处理呢？按照顺时针⽅向找最近的服务器节点  
![⼀致性哈希算法2](/assets/lagou/第二阶段/02.第二模块/⼀致性哈希算法2.jpg)

假如将服务器3下线，服务器3下线后，原来路由到3的客户端重新路由到服务器4，对于其他客户端没有影响只是这⼀⼩部分受影响（请求的迁移达到了最⼩，这样的算法对分布式集群来说⾮常合适的，避免了⼤量请求迁移 ）  
![⼀致性哈希算法3](/assets/lagou/第二阶段/02.第二模块/⼀致性哈希算法3.jpg)

增加服务器5之后，原来路由到3的部分客户端路由到新增服务器5上，对于其他客户端没有影响只是这⼀⼩部分受影响（请求的迁移达到了最⼩，这样的算法对分布式集群来说⾮常合适的，避免了⼤量请求迁移 ）  
![⼀致性哈希算法4](/assets/lagou/第二阶段/02.第二模块/⼀致性哈希算法4.jpg)

1)如前所述，每⼀台服务器负责⼀段，⼀致性哈希算法对于节点的增减都只需重定位环空间中的⼀⼩部分数据，具有较好的容错性和可扩展性  

但是，⼀致性哈希算法在服务节点太少时，容易因为节点分部不均匀⽽造成数据倾斜问题。例如系统中只有两台服务器，其环分布如下，节点2只能负责⾮常⼩的⼀段，⼤量的客户端  请求落在了节点1上，这就是数据（请求）倾斜问题

2)为了解决这种数据倾斜问题，⼀致性哈希算法引⼊了虚拟节点机制，即对每⼀个服务节点计算多个哈希，每个计算结果位置都放置⼀个此服务节点，称为虚拟节点。

具体做法可以在服务器ip或主机名的后⾯增加编号来实现。⽐如，可以为每台服务器计算三个虚拟节点，于是可以分别计算 “节点1的ip#1”、 “节点1的ip#2”、 “节点1的ip#3”、“节点2的ip#1”、“节点2的ip#2”、“节点2的ip#3”的哈希值，于是形成六个虚拟节点，当客户端被路由到虚拟节点的时候其实是被路由到该虚拟节点所对应的真实节点  
![⼀致性哈希算法5](/assets/lagou/第二阶段/02.第二模块/⼀致性哈希算法5.jpg)

## 1.4 Nginx 配置⼀致性Hash负载均衡策略

ngx_http_upstream_consistent_hash 模块是⼀个负载均衡器，使⽤⼀个内部⼀致性hash算法来选择合适的后端节点。

该模块可以根据配置参数采取不同的⽅式将请求均匀映射到后端机  
>consistent_hash $remote_addr：可以根据客户端ip映射  
>consistent_hash $request_uri：根据客户端请求的uri映射  
>consistent_hash $args：根据客户端携带的参数进⾏映    
>ngx_http_upstream_consistent_hash 模块是⼀个第三⽅模块，需要我们下载安装后使⽤

1)github下载nginx⼀致性hash负载均衡模块 [https://github.com/replay/ngx_http_consistent_hash](https://github.com/replay/ngx_http_consistent_hash)  
2)将下载的压缩包上传到nginx服务器，并解压  
3)我们已经编译安装过nginx，此时进⼊当时nginx的源码⽬录，执⾏如下命令

>/configure —add-module=/root/ngx_http_consistent_hash-master
>make
>make install

4)Nginx就可以使⽤啦，在nginx.conf⽂件中配置    
![⼀致性哈希算法nginx](/assets/lagou/第二阶段/02.第二模块/⼀致性哈希算法nginx.jpg)

# 第⼆部分 集群时钟同步问题
## 2.1 时钟不同步导致的问题
时钟此处指服务器时间，如果集群中各个服务器时钟不⼀致势必导致⼀系列问题，试想 集群是各个服务器⼀起团队化作战，⼤家⼯作都不在⼀个点上，岂不乱了套！

举⼀个例⼦，电商⽹站业务中，新增⼀条订单，那么势必会在订单表中增加了⼀条记录，该条记录中应该会有“下单时间”这样的字段，往往我们会在程序中获取当前系统时间插⼊到数据库或者直接从数据库服务器获取时间。那我们的订单⼦系统是集群化部署，或者我们的数据库也是分库分表的集群化部署，然⽽他们的系统时钟缺不⼀致，⽐如有⼀台服务器的时间是昨天，那么这个时候下单时间就成了昨天，那我们的数据将会混乱！如下

![时钟不同步](/assets/lagou/第二阶段/02.第二模块/时钟不同步.jpg)

## 2.2 集群时钟同步配置
**集群时钟同步思路**

* 分布式集群中各个服务器节点都可以连接互联⽹,各个节点从国家授时中心获取时间

操作⽅式：
```
#使⽤ ntpdate ⽹络时间同步命令
ntpdate -u ntp.api.bz #从⼀个时间服务器同步时间
```

* 分布式集群中某⼀个服务器节点可以访问互联⽹或者所有节点都不能够访问互联⽹

操作⽅式：  
选取集群中的⼀个服务器节点A(172.17.0.17)作为时间服务器（整个集群时间从这台服务器同步，如果这台服务器能够访问互联⽹，可以让这台服务器和⽹络时间保持同步，如果不能就⼿动设置⼀个时间）

* 1.⾸先设置好A的时间  
* 2.把A配置为时间服务器（修改/etc/ntp.conf⽂件）  

```
1、如果有 restrict default ignore，注释掉它
2、添加如下⼏⾏内容
    restrict 172.17.0.0 mask 255.255.255.0 nomodify notrap # 放开局域⽹同步功能,172.17.0.0是你的局域⽹⽹段
    server 127.127.1.0 # local clock
    fudge 127.127.1.0 stratum 10
3、重启⽣效并配置ntpd服务开机⾃启动
    service ntpd restart
    chkconfig ntpd on
```

* 集群中其他节点就可以从A服务器同步时间了

```
ntpdate 172.17.0.17
```

# 第三部分 分布式ID解决⽅案
为什么需要分布式ID(分布式集群环境下的全局唯⼀ID)  
![分布式ID](/assets/lagou/第二阶段/02.第二模块/分布式ID.jpg)

## 3.1 UUID(可以⽤)
## 3.2 独⽴数据库的⾃增ID
⽐如A表分表为A1表和A2表，那么肯定不能让A1表和A2表的ID⾃增，那么ID怎么获取呢？我们可以单独的创建⼀个Mysql数据库，在这个数据库中创建⼀张表，这张表的ID设置为⾃增，其他地⽅需要全局唯⼀ID的时候，就模拟向这个Mysql数据库的这张表中模拟插⼊⼀条记录，此时ID会⾃增，然后我们可以通过Mysql的select last_insert_id() 获取到刚刚这张表中⾃增⽣成的ID.

## 3.3 SnowFlake雪花算法(可以⽤，推荐)
雪花算法是Twitter推出的⼀个⽤于⽣成分布式ID的策略。  
雪花算法是⼀个算法，基于这个算法可以⽣成ID，⽣成的ID是⼀个long型，那么在Java中⼀个long型是8个字节，算下来是64bit，如下是使⽤雪花算法⽣成的⼀个ID的⼆进制形式示意：  
![雪花算法](/assets/lagou/第二阶段/02.第二模块/雪花算法.jpg)

另外，⼀切互联⽹公司也基于上述的⽅案封装了⼀些分布式ID⽣成器，⽐如滴滴的tinyid（基于数据库实现）、百度的uidgenerator（基于SnowFlake）和美团的leaf（基于数据库和SnowFlake）等，他们在。

## 3.4 借助Redis的Incr命令获取全局唯⼀ID(推荐)
Redis Incr 命令将 key 中储存的数字值增⼀。如果 key 不存在，那么 key 的值会先被初始化为0，然后再执⾏ INCR 操作。   
![借助Redis的Incr命令](/assets/lagou/第二阶段/02.第二模块/借助Redis的Incr命令.jpg)


















































































